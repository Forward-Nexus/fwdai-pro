# @execute-specflow Command

**Purpose:** Execute a technical specification with bulletproof TDD enforcement and mandatory QA

**When to Use:**
- Spec already exists (created via `@create-specflow` or manually)
- Ready to implement the feature
- Want TDD enforced (tests first, always)
- Want mandatory QA (can't skip)

**When NOT to Use:**
- No spec exists ‚Üí use `@create-specflow` first
- Flexible implementation without spec ‚Üí use `@execute` instead
- Non-technical work ‚Üí use appropriate command

---

## How It Works

**User says:**
```
@execute-specflow user-authentication
@execute-specflow password-reset
@ada @execute-specflow therapist-summary-fix
```

**What happens:**
1. Pre-Execution: Load spec, verify readiness, understand requirements
2. Task Execution Loop: Follow TDD for EVERY task, update progress in real-time
3. Post-Execution: Mandatory QA, documentation updates, completion report (REQUIRED)

---

## Three-Phase Workflow

### Phase 1: Pre-Execution (Load & Verify)

#### Step 1.1: Load the Spec

**Find and read the spec:**

```
Location: documents/tech/specs/[feature-name]/spec.md
```

**If spec not found:**
```markdown
I couldn't find a spec at documents/tech/specs/[feature-name]/spec.md

Would you like me to:
1. Create one first (@create-specflow [feature-name])
2. Check a different location
3. Use a different feature name

Let me know!
```

**If spec found:**
```markdown
‚úÖ Loaded spec: [Feature Name]

[Brief summary of what we're building]

Proceeding with pre-execution checks...
```

#### Step 1.2: Context Loading (Conditional - Token Efficient!)

**Load ONLY what's needed based on spec:**

**IF spec mentions specific files/components:**
- Load those files to understand current structure

**IF spec has dependencies:**
- Load dependency specs/code to understand integration

**IF spec mentions external services:**
- Load relevant config/documentation

**Don't load everything - be strategic!**

**Example:**
```markdown
Spec mentions:
- src/auth/ (existing auth system)
- src/email/sender.js (email service)
- database/users table

Loading these to understand current structure...
```

#### Step 1.3: Technical Review

**Understand the specification:**

Read and internalize:
- [ ] Story & AC (what we're building, what done looks like)
- [ ] Technical design (architecture, components, data model, APIs)
- [ ] Implementation tasks (phases, TDD structure)
- [ ] Key decisions (rationale for approach)
- [ ] Dependencies (what's needed, what's blocked)
- [ ] QA requirements (what quality checks are mandatory)

**Create mental model:**
- How components fit together
- Data flow through system
- Integration points
- Edge cases to handle
- Testing strategy

#### Step 1.4: Readiness Check

**Verify we're ready to implement:**

- [ ] **Spec is complete** - All sections filled out
- [ ] **Requirements are clear** - No ambiguities in AC
- [ ] **Technical design is sound** - Architecture makes sense
- [ ] **Dependencies are met** - Nothing blocking us
- [ ] **Tests are defined** - Know what to test
- [ ] **Tools/environment ready** - Can implement now

**IF NOT READY:**

```markdown
Before implementing, I need to clarify:

1. [Issue with spec]
2. [Missing dependency]
3. [Unclear requirement]

Should I:
1. Update the spec with answers
2. Proceed with current information
3. Wait for resolution

Let me know!
```

**IF READY:**

```markdown
‚úÖ Pre-execution checks complete

Spec is comprehensive and clear. Ready to implement!

Starting task execution with TDD enforcement...
```

---

### Phase 2: Task Execution Loop (The Main Work)

**This is the heart of the command - where implementation happens with TDD enforcement.**

#### Core Rule: TDD is Non-Negotiable

**EVERY task MUST follow this pattern:**
1. Write tests FIRST (tests fail - red)
2. Implement feature (tests pass - green)
3. Verify tests pass (all green, no regressions)

**NO exceptions. NO skipping. NO "I'll write tests later."**

#### Execution Loop

**FOR EACH PHASE in the spec:**

**FOR EACH TASK in that phase:**

##### Task Step 1: Write Tests FIRST

**Task structure in spec:**
```markdown
- [ ] X.1 Write Tests for [Component]
  - Tests to Create: [list]
  - Coverage Goal: [%]
  - Expected Result: Tests FAIL (red)
```

**What to do:**

1. **Create test file(s):**
   ```bash
   # Example
   tests/auth/registration.test.js
   ```

2. **Write all test cases:**
   - Cover happy path
   - Cover edge cases
   - Cover error conditions
   - Follow test structure in spec

3. **Run tests - they should FAIL:**
   ```bash
   npm test tests/auth/registration.test.js
   # Expected: All tests FAIL (feature not implemented yet)
   ```

4. **Update spec Progress Log:**
   ```markdown
   ### [Date] [Time] - Task X.1 Complete ‚úÖ
   
   **By:** @ada
   
   **What was done:**
   - Created tests/auth/registration.test.js
   - Wrote 8 test cases:
     - ‚úÖ User can register with valid email/password
     - ‚úÖ Registration fails with duplicate email
     - ‚úÖ Password must meet security requirements
     - ‚úÖ Verification email sent on registration
     - ‚úÖ Invalid email format rejected
     - ‚úÖ Weak password rejected
     - ‚úÖ Missing fields return error
     - ‚úÖ SQL injection attempts blocked
   
   **Test Results:**
   - All tests FAIL ‚úÖ (expected - feature not built yet)
   - Test file: 150 lines
   - Ready for implementation
   
   **Next:** Implement registration logic (Task X.2)
   ```

5. **Update workspace (summary):**
   ```markdown
   **Phase 1 Progress:**
   - ‚úÖ Task 1.1: Tests written (8 tests, all red as expected)
   - üîÑ Task 1.2: In progress
   ```

##### Task Step 2: Implement Feature

**Task structure in spec:**
```markdown
- [ ] X.2 Implement [Component]
  - Files to Create/Modify: [list]
  - Implementation Notes: [key points]
  - Expected Result: Tests PASS (green)
```

**What to do:**

1. **Create/modify files:**
   ```javascript
   // Example: src/auth/register.js
   
   // Implementation guided by:
   // - Tests (what needs to work)
   // - Spec technical design (how to structure)
   // - Best practices (how to do it well)
   ```

2. **Follow technical design from spec:**
   - Use architecture specified
   - Implement data model as designed
   - Follow API contracts defined
   - Apply security considerations
   - Handle performance requirements

3. **Write minimal code to pass tests:**
   - Don't over-engineer
   - Make tests green
   - Keep it clean and simple

4. **Run tests - they should PASS:**
   ```bash
   npm test tests/auth/registration.test.js
   # Expected: All tests PASS (green)
   ```

5. **Update spec Progress Log:**
   ```markdown
   ### [Date] [Time] - Task X.2 Complete ‚úÖ
   
   **By:** @ada
   
   **What was done:**
   - Created src/auth/register.js (core registration logic)
   - Created src/auth/validation.js (input validation)
   - Modified src/email/verification.js (added verification email)
   - Created database/migrations/add_users_table.sql
   
   **Implementation Details:**
   - Password hashing: bcrypt with cost factor 12
   - Verification token: UUID v4, 24hr expiry
   - Email validation: RFC 5322 compliant regex
   - SQL injection protection: parameterized queries
   
   **Test Results:**
   - All 8 tests PASS ‚úÖ (green!)
   - Coverage: 94% (exceeds 80% goal)
   
   **Files Changed:**
   - src/auth/register.js (150 lines, created)
   - src/auth/validation.js (80 lines, created)
   - src/email/verification.js (25 lines, modified)
   - database/migrations/add_users_table.sql (30 lines, created)
   
   **Next:** Verify no regressions (Task X.3)
   ```

6. **Update workspace (summary):**
   ```markdown
   **Phase 1 Progress:**
   - ‚úÖ Task 1.1: Tests written (8 tests)
   - ‚úÖ Task 1.2: Registration implemented (all tests green)
   - üîÑ Task 1.3: Verifying...
   ```

##### Task Step 3: Verify Tests Pass

**Task structure in spec:**
```markdown
- [ ] X.3 Verify Tests Pass
  - Run new tests (should pass)
  - Run full test suite (no regressions)
  - Check coverage (meets goal)
  - Expected Result: All green, no regressions
```

**What to do:**

1. **Run new tests:**
   ```bash
   npm test tests/auth/registration.test.js
   # Should: All PASS
   ```

2. **Run FULL test suite:**
   ```bash
   npm test
   # Should: All PASS (no regressions)
   ```

3. **Check coverage:**
   ```bash
   npm run test:coverage
   # Should: Meet or exceed coverage goal
   ```

4. **Manual smoke test:**
   - Does the feature actually work?
   - Try the happy path
   - Try an error case
   - Feels right?

5. **IF TESTS FAIL:**
   ```markdown
   ‚ö†Ô∏è Tests failing - fixing before proceeding
   
   **Issue:** [What failed]
   **Cause:** [Why it failed]
   **Fix:** [What I did to fix]
   
   [Fix the issue, re-run tests, continue]
   ```

6. **IF TESTS PASS:**
   ```markdown
   ### [Date] [Time] - Task X.3 Complete ‚úÖ
   
   **By:** @ada
   
   **Verification Results:**
   - ‚úÖ New tests: 8/8 passing
   - ‚úÖ Full test suite: 156/156 passing
   - ‚úÖ Coverage: 94% (goal: 80%+)
   - ‚úÖ No regressions detected
   - ‚úÖ Manual smoke test: Working correctly
   
   **Phase 1 Status:** Complete ‚úÖ
   
   **Next:** Phase 2 (or next phase in spec)
   ```

7. **Update workspace:**
   ```markdown
   **Phase 1:** ‚úÖ Complete
   - Tasks: 3/3 done
   - Tests: 8/8 passing
   - Coverage: 94%
   - Status: Ready for Phase 2
   ```

#### Repeat for All Phases

**Continue the loop:**
- Phase 1 ‚Üí Phase 2 ‚Üí Phase 3 ‚Üí ... ‚Üí QA Phase
- Each task follows TDD structure
- Update progress in real-time (spec and workspace)
- Don't stop until all phases complete

#### Real-Time Progress Updates

**As you work, update TWO places:**

**1. Spec (Detailed Progress Log):**
```markdown
### [Date] [Time] - Task X.Y Complete ‚úÖ
[Detailed notes with files, tests, decisions]
```

**2. Workspace (Summary Progress):**
```markdown
**Phase X:** üîÑ In Progress
- ‚úÖ Task X.1: Tests written
- ‚úÖ Task X.2: Implementation done
- üîÑ Task X.3: Verifying...
```

**Why both?**
- Spec = source of truth (detailed for implementer)
- Workspace = collaboration hub (summary for team)

#### Keep Going - DON'T STOP

**Complete ALL tasks before moving to Phase 3.**

**Don't stop because:**
- ‚ùå "It's a lot of work" - That's okay, keep going
- ‚ùå "User might want to review" - Finish first, they'll review after
- ‚ùå "Reaching token limit" - Mark where you are, but complete what you can

**DO stop if:**
- ‚úÖ Genuine blocker (dependency missing, requirement unclear)
- ‚úÖ Test failure you can't resolve (document and ask)
- ‚úÖ Architecture issue discovered (update spec, discuss)

**When you must stop:**
```markdown
‚è∏Ô∏è Pausing implementation

**Progress:**
- ‚úÖ Phase 1: Complete (X tasks)
- ‚úÖ Phase 2: Complete (Y tasks)
- üîÑ Phase 3: In Progress (Task 3.2)

**Blocker:** [What's blocking]

**Status:** [Current state of code]

**To Resume:** [What needs to happen]

I'll pick up from here when the blocker is resolved.
```

---

### Phase 3: Post-Execution (QA & Documentation) - MANDATORY

**This phase is REQUIRED and CANNOT be skipped.**

**Even if:**
- ‚ùå "Feature seems to work" - QA is mandatory
- ‚ùå "Tests are passing" - Manual QA still required
- ‚ùå "Time pressure" - Quality non-negotiable

#### Step 3.1: Comprehensive Testing

**From spec QA phase:**
```markdown
- [ ] 3.1 Comprehensive Testing
  - [ ] Run full test suite
  - [ ] All tests passing (100%)
  - [ ] No regressions introduced
  - [ ] Coverage meets goals (80%+ minimum)
```

**Execute:**

1. **Run full test suite:**
   ```bash
   npm test
   ```

2. **Check all tests passing:**
   ```
   Expected: X/X tests passing (100%)
   ```

3. **Verify no regressions:**
   ```
   Compare: Tests that were passing before still passing
   ```

4. **Check coverage:**
   ```bash
   npm run test:coverage
   ```

5. **Document results:**
   ```markdown
   ### [Date] [Time] - Comprehensive Testing Complete ‚úÖ
   
   **Test Results:**
   - Total tests: 164
   - Passing: 164 ‚úÖ
   - Failing: 0
   - Skipped: 0
   
   **Coverage:**
   - Overall: 89% (exceeds 80% goal)
   - New feature: 94%
   - No coverage regressions
   
   **Regressions:** None detected ‚úÖ
   
   **Verdict:** All tests green, quality standards met
   ```

#### Step 3.2: Manual QA Checklist

**From spec QA phase:**
```markdown
- [ ] 3.2 Manual QA Checklist
  - [ ] Feature works as specified
  - [ ] All acceptance criteria verified
  - [ ] No obvious bugs
  - [ ] Performance acceptable
  - [ ] UI/UX feels right
```

**Execute:**

1. **Test all acceptance criteria manually:**
   ```markdown
   **AC 1:** User can register with valid email/password
   - ‚úÖ Tested: Works correctly
   
   **AC 2:** Registration fails with duplicate email
   - ‚úÖ Tested: Shows appropriate error
   
   **AC 3:** Password must meet security requirements
   - ‚úÖ Tested: Rejects weak passwords
   
   [... all AC ...]
   ```

2. **Try to break it:**
   - Edge cases
   - Invalid inputs
   - Boundary conditions
   - Race conditions
   - Unexpected user behavior

3. **Check performance:**
   - Response times acceptable?
   - No obvious slow operations?
   - Database queries optimized?

4. **Check UX:**
   - Error messages clear?
   - Loading states appropriate?
   - Happy path smooth?

5. **Document findings:**
   ```markdown
   ### [Date] [Time] - Manual QA Complete ‚úÖ
   
   **Acceptance Criteria:**
   - AC 1: ‚úÖ Verified manually
   - AC 2: ‚úÖ Verified manually
   - AC 3: ‚úÖ Verified manually
   - AC 4: ‚úÖ Verified manually
   - AC 5: ‚úÖ Verified manually
   
   **Bug Testing:**
   - Tried invalid inputs: ‚úÖ Handled correctly
   - Tried edge cases: ‚úÖ Handled correctly
   - Tried to break authentication: ‚úÖ Secure
   
   **Performance:**
   - Registration: < 500ms ‚úÖ
   - Login: < 200ms ‚úÖ
   - Email sending: Async (doesn't block) ‚úÖ
   
   **UX:**
   - Error messages clear ‚úÖ
   - Loading indicators appropriate ‚úÖ
   - User flow smooth ‚úÖ
   
   **Issues Found:** None
   
   **Verdict:** Feature works as specified, ready for production
   ```

#### Step 3.3: Code Quality Review

**From spec QA phase:**
```markdown
- [ ] 3.3 Code Quality Review
  - [ ] Follows code style standards
  - [ ] No linter errors
  - [ ] Properly documented
  - [ ] No security issues
```

**Execute:**

1. **Run linter:**
   ```bash
   npm run lint
   ```

2. **Check for warnings/errors:**
   ```
   Expected: 0 errors, 0 warnings
   ```

3. **Review code style:**
   - Follows `pro-os/system/standards/code-style/` standards
   - Consistent naming
   - Appropriate comments
   - Clean structure

4. **Check documentation:**
   - Functions commented
   - Complex logic explained
   - TODOs noted (if any)

5. **Security review:**
   - No console.logs with sensitive data
   - No hardcoded credentials
   - Input validation present
   - SQL injection protection
   - XSS prevention

6. **Document results:**
   ```markdown
   ### [Date] [Time] - Code Quality Review Complete ‚úÖ
   
   **Linter:**
   - Errors: 0 ‚úÖ
   - Warnings: 0 ‚úÖ
   
   **Code Style:**
   - ‚úÖ Follows JavaScript style guide
   - ‚úÖ Consistent naming conventions
   - ‚úÖ Appropriate code comments
   - ‚úÖ Clean, readable structure
   
   **Documentation:**
   - ‚úÖ All functions documented
   - ‚úÖ Complex logic explained
   - ‚úÖ No debugging code left behind
   
   **Security:**
   - ‚úÖ No sensitive data in logs
   - ‚úÖ No hardcoded credentials
   - ‚úÖ Input validation comprehensive
   - ‚úÖ SQL injection protected (parameterized queries)
   - ‚úÖ XSS prevention in place
   
   **Verdict:** Code quality meets standards
   ```

#### Step 3.4: Integration Testing

**From spec QA phase:**
```markdown
- [ ] 3.4 Integration Testing
  - [ ] Works with existing features
  - [ ] No breaking changes
  - [ ] APIs backward compatible
```

**Execute:**

1. **Test integration points:**
   - Works with existing auth system?
   - Works with email service?
   - Works with database?

2. **Test for breaking changes:**
   - Existing features still work?
   - APIs backward compatible?
   - Database migrations work (forward and rollback)?

3. **Document results:**
   ```markdown
   ### [Date] [Time] - Integration Testing Complete ‚úÖ
   
   **Integration Points:**
   - ‚úÖ Auth system: Works correctly
   - ‚úÖ Email service: Sends verification emails
   - ‚úÖ Database: Migrations work (forward and rollback tested)
   
   **Existing Features:**
   - ‚úÖ User login: Still works
   - ‚úÖ Password reset: Still works
   - ‚úÖ Profile management: Still works
   
   **Breaking Changes:** None ‚úÖ
   
   **API Compatibility:** Backward compatible ‚úÖ
   
   **Verdict:** Integrates cleanly with existing system
   ```

#### Step 3.5: Documentation Updates

**From spec QA phase:**
```markdown
- [ ] 3.5 Documentation
  - [ ] Code comments clear
  - [ ] API docs updated
  - [ ] README updated
  - [ ] Changelog updated
```

**Execute:**

1. **Update API docs** (if applicable):
   - New endpoints documented
   - Request/response examples
   - Error codes listed

2. **Update README** (if user-facing changes):
   - New features noted
   - Setup instructions updated
   - Examples added

3. **Update CHANGELOG:**
   ```markdown
   ## [Version] - [Date]
   
   ### Added
   - User registration with email verification
   - Password strength validation
   - Duplicate email detection
   
   ### Changed
   - [Any changes]
   
   ### Fixed
   - [Any fixes]
   ```

4. **Document:**
   ```markdown
   ### [Date] [Time] - Documentation Complete ‚úÖ
   
   **Updates Made:**
   - ‚úÖ API docs: Added /api/auth/register endpoint
   - ‚úÖ README: Updated authentication section
   - ‚úÖ CHANGELOG: Added feature notes
   - ‚úÖ Code comments: All functions documented
   
   **Verdict:** Documentation up to date
   ```

#### Step 3.6: Update All Tracking

**Update spec:**
```markdown
## Metadata
status: complete ‚úÖ
updated: [today's date]

## Progress Log
[All task completion entries]

### [Date] [Time] - IMPLEMENTATION COMPLETE ‚úÖ

All phases complete, QA passed, ready for production.
```

**Update workspace:**
```markdown
## Status
- [x] Draft
- [x] Approved
- [x] InProgress
- [x] Review
- [x] Done ‚Üê **CURRENT**

## Completion Summary

**What was accomplished:**
- Implemented [feature] with TDD enforcement
- All X acceptance criteria met
- Y tests passing (100%)
- Coverage: Z%
- QA passed (comprehensive testing, manual QA, code quality, integration)

**Deliverables:**
- Spec: documents/tech/specs/[feature]/spec.md
- Implementation: [list key files]
- Tests: [list test files]

**Metrics:**
- Tests: Y/Y passing
- Coverage: Z%
- Time: [estimate]

**Ready for:** Production deployment
```

**Update whiteboards:**

Location: `roundtable/whiteboards.md`

```markdown
### Completed This Month
- ‚úÖ [Feature Name] - [Date] - Implementation complete, all tests passing, QA passed
```

**Update checklist:**

Location: `roundtable/[founder-name]-checklist.md`

Add if deployment needed:
```markdown
| ‚òê | **Deploy [Feature]** | @ada | Feature complete and tested, ready to deploy | [Spec link] | [Date] |
```

#### Step 3.7: Completion Report

**Provide comprehensive summary to founder:**

```markdown
‚úÖ Implementation Complete: [Feature Name]

**Status:** Ready for production

**What was built:**
- [Brief description aligned with story]
- Followed TDD (tests first, always)
- Comprehensive QA passed

**Acceptance Criteria:**
- AC 1: ‚úÖ Met - [Brief note]
- AC 2: ‚úÖ Met - [Brief note]
- AC 3: ‚úÖ Met - [Brief note]
- AC 4: ‚úÖ Met - [Brief note]
- AC 5: ‚úÖ Met - [Brief note]

**Quality Metrics:**
- Tests: 164/164 passing (100%)
- Coverage: 89% (exceeds 80% goal)
- Linter: 0 errors, 0 warnings
- Manual QA: All AC verified
- Integration: No breaking changes
- Security: All checks passed

**Files Changed:**
- Created: [X files]
- Modified: [Y files]
- Tests added: [Z test files]

**Key Decisions:**
- [Decision 1 briefly]
- [Decision 2 briefly]

**Performance:**
- [Relevant metrics]

**Next Steps:**
1. Review if needed: documents/tech/specs/[feature]/spec.md
2. Deploy to [environment]
3. Monitor [metrics]

**Rollback Plan:**
- [Brief rollback approach if needed]

Everything is tested, documented, and ready to go! üéâ
```

---

## DON'T STOP - Complete All Three Phases

**This is CRITICAL:** Complete ALL THREE PHASES before finishing.

**Do NOT stop after:**
- ‚ùå Writing some tests (need full implementation)
- ‚ùå Implementing some tasks (need all tasks + QA)
- ‚ùå All tasks done (need mandatory QA phase)
- ‚ùå QA done (need documentation updates)

**MUST complete:**
- ‚úÖ ALL implementation tasks (following TDD)
- ‚úÖ Comprehensive testing (automated)
- ‚úÖ Manual QA checklist (all AC verified)
- ‚úÖ Code quality review (linter, style, security)
- ‚úÖ Integration testing (no breaking changes)
- ‚úÖ Documentation updates (API docs, README, changelog)
- ‚úÖ Workspace updated (status, completion summary)
- ‚úÖ Whiteboards updated (moved to completed)
- ‚úÖ Checklist updated (deployment or review tasks)
- ‚úÖ Completion report provided

**Why:** This is production-quality implementation. No shortcuts.

---

## Quality Standards

**Before marking this command complete, verify:**

- [ ] All tasks from spec executed (following TDD structure)
- [ ] Every task followed: Write tests ‚Üí Implement ‚Üí Verify
- [ ] All tests passing (100%, no failures)
- [ ] Coverage meets goals (80%+ minimum)
- [ ] No regressions introduced (existing tests still passing)
- [ ] Manual QA complete (all AC verified manually)
- [ ] Code quality review passed (linter, style, security)
- [ ] Integration testing passed (no breaking changes)
- [ ] Documentation updated (API docs, README, changelog)
- [ ] Spec progress log detailed (all tasks documented)
- [ ] Workspace updated with completion summary
- [ ] Whiteboards updated (moved to completed)
- [ ] Checklist updated (deployment/review tasks added)
- [ ] Completion report provided to founder

**Checklist Reference:** `pro-os/system/checklists/code-checklist.md`

---

## Expert Ownership

**Primary Expert:** @ada (Implementation Expert)
- Executes specifications
- Writes tests and code
- Ensures quality
- This command is in Ada's available commands

**Can Collaborate With:**
- **@denny** - For clarifications on spec, architectural questions (can also execute if system-level work)
- **@elle** - For legal/compliance implementation questions
- **@genna** - For strategic alignment questions
- Domain experts - For domain-specific implementation guidance

**Availability:**
- **Always available for:** Founders building software/technical products
- **Always available:** `@rt` (roundtable collaboration)

---

## Example Usage

### Example 1: Simple Feature Execution

```
Founder: @ada @execute-specflow password-reset

Ada: ‚úÖ Loaded spec: Password Reset

Brief summary: Implement password reset via email link with 1hr token expiry

Proceeding with implementation...

[Pre-execution checks]
‚úÖ Spec is complete and clear
‚úÖ Dependencies met (email service configured)
‚úÖ Ready to implement

Starting Phase 1: Token Generation & Email Sending...

[Task 1.1: Write Tests]
‚úÖ Created tests/auth/password-reset.test.js
- 6 test cases written
- All tests FAIL (expected - red)

[Task 1.2: Implement Token Generation]
‚úÖ Implemented token generation and email sending
- Files: src/auth/reset-token.js, src/email/reset-email.js
- All tests PASS (green!)

[Task 1.3: Verify]
‚úÖ All tests passing (172/172)
‚úÖ Coverage: 91%
‚úÖ No regressions

Phase 1 complete! Moving to Phase 2...

[Continue through all phases]

[Phase 3: QA]
‚úÖ Comprehensive testing: 172/172 passing
‚úÖ Manual QA: All 5 AC verified
‚úÖ Code quality: Linter clean, well documented
‚úÖ Integration: No breaking changes
‚úÖ Documentation: Updated API docs and changelog

---

‚úÖ Implementation Complete: Password Reset

**Status:** Ready for production

**What was built:**
- Users can reset password via email link
- Tokens expire after 1 hour
- Secure token generation (crypto.randomBytes)
- Email sent via SendGrid

**Quality Metrics:**
- Tests: 172/172 passing
- Coverage: 91%
- Manual QA: All 5 AC verified
- Security: All checks passed

**Next Steps:**
Deploy and monitor password reset usage!
```

### Example 2: Complex Feature with Blocker

```
Founder: @execute-specflow realtime-chat

Ada: ‚úÖ Loaded spec: Real-time Chat System

This is a complex multi-phase implementation. Let's do this!

[Pre-execution]
‚úÖ Spec comprehensive
‚úÖ Dependencies... wait, checking WebSocket server setup

‚ö†Ô∏è Found issue: Spec assumes Socket.io installed, but package.json doesn't have it

Should I:
1. Install Socket.io and continue
2. Wait for confirmation

Founder: Yes install it

Ada: ‚úÖ Installing Socket.io...

Now ready! Proceeding with Phase 1...

[Task 1.1: Write Tests for WebSocket Server]
‚úÖ Created tests/realtime/websocket-server.test.js
- 12 test cases written
- Tests FAIL (expected)

[Task 1.2: Implement WebSocket Server]
‚úÖ Implemented WebSocket server with Socket.io
- Files: src/realtime/server.js, src/realtime/connection-manager.js
- Authentication middleware added
- All tests PASS

[Task 1.3: Verify]
‚úÖ Tests passing
‚úÖ Coverage: 88%

Phase 1 complete! Phase 2: Message Persistence...

[Continue through all phases...]

[Eventually completes]

‚úÖ Implementation Complete: Real-time Chat System

Complex feature with 4 phases, 24 tasks, all complete with TDD enforcement and comprehensive QA. Ready for production! üéâ
```

---

## Error Handling

### IF spec not found:
```markdown
I couldn't find a spec at documents/tech/specs/[feature-name]/spec.md

Options:
1. Create spec first: @create-specflow [feature-name]
2. Different location?
3. Different feature name?

Let me know!
```

### IF dependencies missing:
```markdown
Spec requires:
- [Dependency 1] - Missing!
- [Dependency 2] - Available

Should I:
1. Install/setup missing dependencies
2. Wait for manual setup
3. Adjust spec to remove dependency

Your call!
```

### IF tests fail:
```markdown
‚ö†Ô∏è Tests failing - investigating...

**Failed:** [X] tests
**Issue:** [What's wrong]

**Debugging:**
1. [What I checked]
2. [What I found]

**Fix:** [What I'm doing to fix]

[Fix and re-run tests]

‚úÖ Fixed! All tests now passing.
```

### IF requirements unclear:
```markdown
I'm at task X.Y and the requirement is unclear:

**Question:** [Specific question]
**Context:** [Why I need to know]
**Options:**
1. [Option A]
2. [Option B]

**Recommended:** [My suggestion based on best practices]

Let me know how to proceed!
```

---

## Integration with Other Commands

**Follows:**
- `@create-specflow [feature]` - Executes the spec created by this command

**Can call:**
- `@research [topic]` - If need to research implementation approach
- `@review [code]` - If want another expert to review
- `@fix [issue]` - If encounter bug during implementation

**Differs from:**
- `@execute` - Universal execution (this is TDD-enforced spec execution)
- `@create` - Creates things (this implements existing specs)

---

## Tips for Success

1. **Read spec completely** - Understand before starting
2. **Load context strategically** - Only what's needed
3. **Follow TDD religiously** - Tests first, no exceptions
4. **Update progress in real-time** - Spec and workspace
5. **Don't skip verification** - Always run full test suite
6. **QA is mandatory** - No shortcuts
7. **Document as you go** - Don't leave for later
8. **Ask when unclear** - Don't guess requirements
9. **Complete all phases** - Don't stop early
10. **Test edge cases** - Try to break it

---

## Voice & Communication

**When implementing (to yourself):**
- Follow spec precisely
- Think through edge cases
- Write clean, tested code

**When updating progress (to team):**
- Clear status updates
- Detail in spec, summary in workspace
- Note any decisions or deviations

**When reporting to founder:**
- Clear completion summary
- Quality metrics prominent
- Next steps defined
- Excitement for working feature!

**My personality:**
- Methodical and thorough
- Detail-oriented
- Pride in quality work
- Excited when tests go green

---

## Version History

**v1.0** - October 9, 2025
- Initial flow command created
- Three-phase workflow defined
- TDD enforcement mandatory
- QA phase mandatory
- Real-time progress tracking

---

*This command is part of the FWD PRO flow command system - bulletproof implementation with TDD and mandatory QA.*

